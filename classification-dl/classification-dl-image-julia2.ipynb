{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "using Statistics\r\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\r\n",
    "using Flux\r\n",
    "using Base.Iterators: repeated\r\n",
    "using MLDatasets\r\n",
    "using Plots\r\n",
    "\r\n",
    "#using CuArrays if you want to use GPU"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Load the data\r\n",
    "x_train, y_train = MLDatasets.MNIST.traindata()\r\n",
    "x_valid, y_valid = MLDatasets.MNIST.testdata()\r\n",
    "# Add the channel layer\r\n",
    "x_train = Flux.unsqueeze(x_train, 3)\r\n",
    "x_valid = Flux.unsqueeze(x_valid, 3)\r\n",
    "# Encode labels\r\n",
    "y_train = onehotbatch(y_train, 0:9)\r\n",
    "y_valid = onehotbatch(y_valid, 0:9)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10×10000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  …  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Create the full dataset\r\n",
    "train_data = Flux.DataLoader((x_train, y_train), batchsize=128);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = Chain(\r\n",
    "    # 28x28 => 14x14\r\n",
    "    Conv((5, 5), 1=>8, pad=2, stride=2, relu),\r\n",
    "    # 14x14 => 7x7\r\n",
    "    Conv((3, 3), 8=>16, pad=1, stride=2, relu),\r\n",
    "    # 7x7 => 4x4\r\n",
    "    Conv((3, 3), 16=>32, pad=1, stride=2, relu),\r\n",
    "    # 4x4 => 2x2\r\n",
    "    Conv((3, 3), 32=>32, pad=1, stride=2, relu),\r\n",
    "    \r\n",
    "    # Average pooling on each width x height feature map\r\n",
    "    GlobalMeanPool(),\r\n",
    "    flatten,\r\n",
    "    \r\n",
    "    Dense(32, 10),\r\n",
    "    softmax)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((5, 5), 1 => 8, relu, pad=2, stride=2),  \u001b[90m# 208 parameters\u001b[39m\n",
       "  Conv((3, 3), 8 => 16, relu, pad=1, stride=2),  \u001b[90m# 1_168 parameters\u001b[39m\n",
       "  Conv((3, 3), 16 => 32, relu, pad=1, stride=2),  \u001b[90m# 4_640 parameters\u001b[39m\n",
       "  Conv((3, 3), 32 => 32, relu, pad=1, stride=2),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "  GlobalMeanPool(),\n",
       "  Flux.flatten,\n",
       "  Dense(32, 10),                        \u001b[90m# 330 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ")\u001b[90m                   # Total: 10 arrays, \u001b[39m15_594 parameters, 62.445 KiB."
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "loss_vector = Vector{Float32}()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Float32[]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "loss(x, y) = crossentropy(model(x), y) \r\n",
    "opt = ADAM(); # popular stochastic gradient descent variant\r\n",
    "accuracy(ŷ, y) = mean(onecold(ŷ) .== onecold(y))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "cb = function()\r\n",
    "    @show(loss(x_train, y_train))\r\n",
    "    push!(loss_vector, loss(x_train, y_train)) # callback to show loss\r\n",
    "end"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "#19 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "#only one epoch\r\n",
    "Flux.train!(loss, params(model), train_data, opt, cb = throttle(cb, 10)); #took me ~5 minutes to train on CPU"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.033581298f0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "LoadError",
     "evalue": "UndefVarError: epoch not defined",
     "traceback": [
      "UndefVarError: epoch not defined",
      "",
      "Stacktrace:",
      " [1] (::var\"#19#20\")()",
      "   @ Main .\\In[63]:3",
      " [2] throttled",
      "   @ C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\utils.jl:682 [inlined]",
      " [3] throttled",
      "   @ C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\utils.jl:678 [inlined]",
      " [4] macro expansion",
      "   @ C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:108 [inlined]",
      " [5] macro expansion",
      "   @ C:\\Users\\joaof\\.julia\\packages\\Juno\\n6wyj\\src\\progress.jl:134 [inlined]",
      " [6] train!(loss::Function, ps::Zygote.Params, data::Flux.Data.DataLoader{Tuple{Base.ReshapedArray{FixedPointNumbers.N0f8, 4, Base.ReinterpretArray{FixedPointNumbers.N0f8, 3, UInt8, Array{UInt8, 3}, false}, Tuple{}}, Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}}, Random._GLOBAL_RNG}, opt::ADAM; cb::Flux.var\"#throttled#68\"{Flux.var\"#throttled#64#69\"{Bool, Bool, var\"#19#20\", Int64}})",
      "   @ Flux.Optimise C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:102",
      " [7] top-level scope",
      "   @ In[64]:2",
      " [8] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "loss_vector = Vector{Float32}()\r\n",
    "using Flux: @epochs\r\n",
    "number_epochs = 10\r\n",
    "\r\n",
    "@epochs number_epochs Flux.train!(loss, params(model), train_data, opt, cb = throttle(cb, 10));"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 1\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.039914362f0\n",
      "loss(x_train, y_train) = 0.037166327f0\n",
      "loss(x_train, y_train) = 0.03710796f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 2\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.036577962f0\n",
      "loss(x_train, y_train) = 0.035938393f0\n",
      "loss(x_train, y_train) = 0.03601216f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 3\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.03487561f0\n",
      "loss(x_train, y_train) = 0.033907507f0\n",
      "loss(x_train, y_train) = 0.032398872f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 4\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.03305437f0\n",
      "loss(x_train, y_train) = 0.03248217f0\n",
      "loss(x_train, y_train) = 0.030697536f0\n",
      "loss(x_train, y_train) = 0.030584708f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 5\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.033062167f0\n",
      "loss(x_train, y_train) = 0.029907264f0\n",
      "loss(x_train, y_train) = 0.02586761f0\n",
      "loss(x_train, y_train) = 0.04191599f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 6\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.030708179f0\n",
      "loss(x_train, y_train) = 0.028833525f0\n",
      "loss(x_train, y_train) = 0.027637236f0\n",
      "loss(x_train, y_train) = 0.04163788f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 7\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.029345809f0\n",
      "loss(x_train, y_train) = 0.027717289f0\n",
      "loss(x_train, y_train) = 0.026872747f0\n",
      "loss(x_train, y_train) = 0.03401498f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 8\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.031031681f0\n",
      "loss(x_train, y_train) = 0.026336145f0\n",
      "loss(x_train, y_train) = 0.026465487f0\n",
      "loss(x_train, y_train) = 0.025413202f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 9\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.02606976f0\n",
      "loss(x_train, y_train) = 0.030141035f0\n",
      "loss(x_train, y_train) = 0.022542423f0\n",
      "loss(x_train, y_train) = 0.028606782f0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Epoch 10\n",
      "└ @ Main C:\\Users\\joaof\\.julia\\packages\\Flux\\Zz9RI\\src\\optimise\\train.jl:138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss(x_train, y_train) = 0.025679233f0\n",
      "loss(x_train, y_train) = 0.026533121f0\n",
      "loss(x_train, y_train) = 0.023749458f0\n",
      "loss(x_train, y_train) = 0.02348092f0\n",
      "loss(x_train, y_train) = 0.036273994f0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy(model(x_train), y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy(model(x_valid), y_valid)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "#not so easy to get vector with the throttle due to number of elements per each\r\n",
    "loss_vector\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42-element Vector{Float32}:\n",
       " 0.043970678\n",
       " 0.039934594\n",
       " 0.042436354\n",
       " 0.04153671\n",
       " 0.039914362\n",
       " 0.037166327\n",
       " 0.03710796\n",
       " 0.036577962\n",
       " 0.035938393\n",
       " 0.03601216\n",
       " 0.03487561\n",
       " 0.033907507\n",
       " 0.032398872\n",
       " ⋮\n",
       " 0.026336145\n",
       " 0.026465487\n",
       " 0.025413202\n",
       " 0.02606976\n",
       " 0.030141035\n",
       " 0.022542423\n",
       " 0.028606782\n",
       " 0.025679233\n",
       " 0.026533121\n",
       " 0.023749458\n",
       " 0.02348092\n",
       " 0.036273994"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This way we could make a plot on how our loss changes while training. But here our callback makes predictions on all the training data on each batch so It's not the most efficient way for the long run.\r\n",
    "\r\n",
    "It would be better to look at our loss on each epoch. For that we will create a custom training loop.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot(1:42, loss_vector, legend=false)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "epochs = 20\r\n",
    "epochs_loss_list = Vector{Float64}()\r\n",
    "\r\n",
    "for epoch in 1:epochs\r\n",
    "    println(\"epoch nr: \",epoch)\r\n",
    "    for batch in train_data\r\n",
    "    \r\n",
    "        gradient = Flux.gradient(params(model)) do\r\n",
    "          training_loss = loss(batch...)\r\n",
    "          return training_loss\r\n",
    "        end\r\n",
    "\r\n",
    "        Flux.update!(opt, params(model), gradient)\r\n",
    "    end\r\n",
    "    \r\n",
    "    push!(epochs_loss_list, loss(x_train, y_train))\r\n",
    "    IJulia.clear_output(true) #This will clear the output before plotting next epoch.\r\n",
    "    display(plot(1:epoch, epochs_loss_list, legend=false)) #builds on the go\r\n",
    "end"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip590\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip590)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip591\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip590)\" d=\"\nM216.436 1486.45 L2352.76 1486.45 L2352.76 47.2441 L216.436 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip592\">\n    <rect x=\"216\" y=\"47\" width=\"2137\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  701.192,1486.45 701.192,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1231.56,1486.45 1231.56,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1761.93,1486.45 1761.93,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2292.29,1486.45 2292.29,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  216.436,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  701.192,1486.45 701.192,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1231.56,1486.45 1231.56,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1761.93,1486.45 1761.93,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2292.29,1486.45 2292.29,1467.55 \n  \"/>\n<path clip-path=\"url(#clip590)\" d=\"M691.47 1512.56 L709.826 1512.56 L709.826 1516.5 L695.752 1516.5 L695.752 1524.97 Q696.77 1524.62 697.789 1524.46 Q698.807 1524.27 699.826 1524.27 Q705.613 1524.27 708.993 1527.44 Q712.372 1530.62 712.372 1536.03 Q712.372 1541.61 708.9 1544.71 Q705.428 1547.79 699.108 1547.79 Q696.932 1547.79 694.664 1547.42 Q692.419 1547.05 690.011 1546.31 L690.011 1541.61 Q692.095 1542.74 694.317 1543.3 Q696.539 1543.86 699.016 1543.86 Q703.02 1543.86 705.358 1541.75 Q707.696 1539.64 707.696 1536.03 Q707.696 1532.42 705.358 1530.31 Q703.02 1528.21 699.016 1528.21 Q697.141 1528.21 695.266 1528.62 Q693.414 1529.04 691.47 1529.92 L691.47 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1206.25 1543.18 L1213.89 1543.18 L1213.89 1516.82 L1205.58 1518.49 L1205.58 1514.23 L1213.84 1512.56 L1218.52 1512.56 L1218.52 1543.18 L1226.15 1543.18 L1226.15 1547.12 L1206.25 1547.12 L1206.25 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1245.6 1515.64 Q1241.99 1515.64 1240.16 1519.2 Q1238.35 1522.75 1238.35 1529.87 Q1238.35 1536.98 1240.16 1540.55 Q1241.99 1544.09 1245.6 1544.09 Q1249.23 1544.09 1251.04 1540.55 Q1252.87 1536.98 1252.87 1529.87 Q1252.87 1522.75 1251.04 1519.2 Q1249.23 1515.64 1245.6 1515.64 M1245.6 1511.93 Q1251.41 1511.93 1254.46 1516.54 Q1257.54 1521.12 1257.54 1529.87 Q1257.54 1538.6 1254.46 1543.21 Q1251.41 1547.79 1245.6 1547.79 Q1239.79 1547.79 1236.71 1543.21 Q1233.65 1538.6 1233.65 1529.87 Q1233.65 1521.12 1236.71 1516.54 Q1239.79 1511.93 1245.6 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1737.11 1543.18 L1744.75 1543.18 L1744.75 1516.82 L1736.44 1518.49 L1736.44 1514.23 L1744.7 1512.56 L1749.38 1512.56 L1749.38 1543.18 L1757.02 1543.18 L1757.02 1547.12 L1737.11 1547.12 L1737.11 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M1766.51 1512.56 L1784.87 1512.56 L1784.87 1516.5 L1770.79 1516.5 L1770.79 1524.97 Q1771.81 1524.62 1772.83 1524.46 Q1773.85 1524.27 1774.87 1524.27 Q1780.65 1524.27 1784.03 1527.44 Q1787.41 1530.62 1787.41 1536.03 Q1787.41 1541.61 1783.94 1544.71 Q1780.47 1547.79 1774.15 1547.79 Q1771.97 1547.79 1769.7 1547.42 Q1767.46 1547.05 1765.05 1546.31 L1765.05 1541.61 Q1767.13 1542.74 1769.36 1543.3 Q1771.58 1543.86 1774.06 1543.86 Q1778.06 1543.86 1780.4 1541.75 Q1782.74 1539.64 1782.74 1536.03 Q1782.74 1532.42 1780.4 1530.31 Q1778.06 1528.21 1774.06 1528.21 Q1772.18 1528.21 1770.31 1528.62 Q1768.45 1529.04 1766.51 1529.92 L1766.51 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M2271.07 1543.18 L2287.39 1543.18 L2287.39 1547.12 L2265.44 1547.12 L2265.44 1543.18 Q2268.1 1540.43 2272.69 1535.8 Q2277.29 1531.15 2278.47 1529.81 Q2280.72 1527.28 2281.6 1525.55 Q2282.5 1523.79 2282.5 1522.1 Q2282.5 1519.34 2280.56 1517.61 Q2278.64 1515.87 2275.53 1515.87 Q2273.34 1515.87 2270.88 1516.63 Q2268.45 1517.4 2265.67 1518.95 L2265.67 1514.23 Q2268.5 1513.09 2270.95 1512.51 Q2273.41 1511.93 2275.44 1511.93 Q2280.81 1511.93 2284.01 1514.62 Q2287.2 1517.31 2287.2 1521.8 Q2287.2 1523.93 2286.39 1525.85 Q2285.6 1527.74 2283.5 1530.34 Q2282.92 1531.01 2279.82 1534.23 Q2276.72 1537.42 2271.07 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M2307.2 1515.64 Q2303.59 1515.64 2301.76 1519.2 Q2299.96 1522.75 2299.96 1529.87 Q2299.96 1536.98 2301.76 1540.55 Q2303.59 1544.09 2307.2 1544.09 Q2310.84 1544.09 2312.64 1540.55 Q2314.47 1536.98 2314.47 1529.87 Q2314.47 1522.75 2312.64 1519.2 Q2310.84 1515.64 2307.2 1515.64 M2307.2 1511.93 Q2313.01 1511.93 2316.07 1516.54 Q2319.15 1521.12 2319.15 1529.87 Q2319.15 1538.6 2316.07 1543.21 Q2313.01 1547.79 2307.2 1547.79 Q2301.39 1547.79 2298.31 1543.21 Q2295.26 1538.6 2295.26 1529.87 Q2295.26 1521.12 2298.31 1516.54 Q2301.39 1511.93 2307.2 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  216.436,1143.56 2352.76,1143.56 \n  \"/>\n<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  216.436,799.727 2352.76,799.727 \n  \"/>\n<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  216.436,455.894 2352.76,455.894 \n  \"/>\n<polyline clip-path=\"url(#clip592)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  216.436,112.06 2352.76,112.06 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  216.436,1486.45 216.436,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  216.436,1143.56 235.334,1143.56 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  216.436,799.727 235.334,799.727 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  216.436,455.894 235.334,455.894 \n  \"/>\n<polyline clip-path=\"url(#clip590)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  216.436,112.06 235.334,112.06 \n  \"/>\n<path clip-path=\"url(#clip590)\" d=\"M62.9365 1129.36 Q59.3254 1129.36 57.4967 1132.92 Q55.6912 1136.47 55.6912 1143.6 Q55.6912 1150.7 57.4967 1154.27 Q59.3254 1157.81 62.9365 1157.81 Q66.5707 1157.81 68.3763 1154.27 Q70.205 1150.7 70.205 1143.6 Q70.205 1136.47 68.3763 1132.92 Q66.5707 1129.36 62.9365 1129.36 M62.9365 1125.66 Q68.7467 1125.66 71.8022 1130.26 Q74.8809 1134.85 74.8809 1143.6 Q74.8809 1152.32 71.8022 1156.93 Q68.7467 1161.51 62.9365 1161.51 Q57.1264 1161.51 54.0477 1156.93 Q50.9921 1152.32 50.9921 1143.6 Q50.9921 1134.85 54.0477 1130.26 Q57.1264 1125.66 62.9365 1125.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M83.0984 1154.96 L87.9827 1154.96 L87.9827 1160.84 L83.0984 1160.84 L83.0984 1154.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M108.168 1129.36 Q104.557 1129.36 102.728 1132.92 Q100.922 1136.47 100.922 1143.6 Q100.922 1150.7 102.728 1154.27 Q104.557 1157.81 108.168 1157.81 Q111.802 1157.81 113.608 1154.27 Q115.436 1150.7 115.436 1143.6 Q115.436 1136.47 113.608 1132.92 Q111.802 1129.36 108.168 1129.36 M108.168 1125.66 Q113.978 1125.66 117.033 1130.26 Q120.112 1134.85 120.112 1143.6 Q120.112 1152.32 117.033 1156.93 Q113.978 1161.51 108.168 1161.51 Q102.358 1161.51 99.2789 1156.93 Q96.2234 1152.32 96.2234 1143.6 Q96.2234 1134.85 99.2789 1130.26 Q102.358 1125.66 108.168 1125.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M129.14 1156.91 L136.779 1156.91 L136.779 1130.54 L128.469 1132.21 L128.469 1127.95 L136.732 1126.28 L141.408 1126.28 L141.408 1156.91 L149.047 1156.91 L149.047 1160.84 L129.14 1160.84 L129.14 1156.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M168.491 1129.36 Q164.88 1129.36 163.052 1132.92 Q161.246 1136.47 161.246 1143.6 Q161.246 1150.7 163.052 1154.27 Q164.88 1157.81 168.491 1157.81 Q172.126 1157.81 173.931 1154.27 Q175.76 1150.7 175.76 1143.6 Q175.76 1136.47 173.931 1132.92 Q172.126 1129.36 168.491 1129.36 M168.491 1125.66 Q174.302 1125.66 177.357 1130.26 Q180.436 1134.85 180.436 1143.6 Q180.436 1152.32 177.357 1156.93 Q174.302 1161.51 168.491 1161.51 Q162.681 1161.51 159.603 1156.93 Q156.547 1152.32 156.547 1143.6 Q156.547 1134.85 159.603 1130.26 Q162.681 1125.66 168.491 1125.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M63.9319 785.526 Q60.3208 785.526 58.4921 789.091 Q56.6865 792.633 56.6865 799.762 Q56.6865 806.869 58.4921 810.433 Q60.3208 813.975 63.9319 813.975 Q67.5661 813.975 69.3717 810.433 Q71.2004 806.869 71.2004 799.762 Q71.2004 792.633 69.3717 789.091 Q67.5661 785.526 63.9319 785.526 M63.9319 781.822 Q69.742 781.822 72.7976 786.429 Q75.8763 791.012 75.8763 799.762 Q75.8763 808.489 72.7976 813.095 Q69.742 817.679 63.9319 817.679 Q58.1217 817.679 55.043 813.095 Q51.9875 808.489 51.9875 799.762 Q51.9875 791.012 55.043 786.429 Q58.1217 781.822 63.9319 781.822 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M84.0938 811.128 L88.978 811.128 L88.978 817.007 L84.0938 817.007 L84.0938 811.128 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M109.163 785.526 Q105.552 785.526 103.723 789.091 Q101.918 792.633 101.918 799.762 Q101.918 806.869 103.723 810.433 Q105.552 813.975 109.163 813.975 Q112.797 813.975 114.603 810.433 Q116.432 806.869 116.432 799.762 Q116.432 792.633 114.603 789.091 Q112.797 785.526 109.163 785.526 M109.163 781.822 Q114.973 781.822 118.029 786.429 Q121.107 791.012 121.107 799.762 Q121.107 808.489 118.029 813.095 Q114.973 817.679 109.163 817.679 Q103.353 817.679 100.274 813.095 Q97.2187 808.489 97.2187 799.762 Q97.2187 791.012 100.274 786.429 Q103.353 781.822 109.163 781.822 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M130.135 813.072 L137.774 813.072 L137.774 786.707 L129.464 788.373 L129.464 784.114 L137.728 782.447 L142.404 782.447 L142.404 813.072 L150.043 813.072 L150.043 817.007 L130.135 817.007 L130.135 813.072 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M159.533 782.447 L177.89 782.447 L177.89 786.383 L163.816 786.383 L163.816 794.855 Q164.834 794.508 165.853 794.346 Q166.871 794.16 167.89 794.16 Q173.677 794.16 177.056 797.332 Q180.436 800.503 180.436 805.92 Q180.436 811.498 176.964 814.6 Q173.491 817.679 167.172 817.679 Q164.996 817.679 162.728 817.308 Q160.482 816.938 158.075 816.197 L158.075 811.498 Q160.158 812.632 162.38 813.188 Q164.603 813.744 167.079 813.744 Q171.084 813.744 173.422 811.637 Q175.76 809.531 175.76 805.92 Q175.76 802.308 173.422 800.202 Q171.084 798.096 167.079 798.096 Q165.204 798.096 163.329 798.512 Q161.478 798.929 159.533 799.808 L159.533 782.447 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M62.9365 441.692 Q59.3254 441.692 57.4967 445.257 Q55.6912 448.799 55.6912 455.928 Q55.6912 463.035 57.4967 466.6 Q59.3254 470.141 62.9365 470.141 Q66.5707 470.141 68.3763 466.6 Q70.205 463.035 70.205 455.928 Q70.205 448.799 68.3763 445.257 Q66.5707 441.692 62.9365 441.692 M62.9365 437.989 Q68.7467 437.989 71.8022 442.595 Q74.8809 447.179 74.8809 455.928 Q74.8809 464.655 71.8022 469.262 Q68.7467 473.845 62.9365 473.845 Q57.1264 473.845 54.0477 469.262 Q50.9921 464.655 50.9921 455.928 Q50.9921 447.179 54.0477 442.595 Q57.1264 437.989 62.9365 437.989 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M83.0984 467.294 L87.9827 467.294 L87.9827 473.174 L83.0984 473.174 L83.0984 467.294 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M108.168 441.692 Q104.557 441.692 102.728 445.257 Q100.922 448.799 100.922 455.928 Q100.922 463.035 102.728 466.6 Q104.557 470.141 108.168 470.141 Q111.802 470.141 113.608 466.6 Q115.436 463.035 115.436 455.928 Q115.436 448.799 113.608 445.257 Q111.802 441.692 108.168 441.692 M108.168 437.989 Q113.978 437.989 117.033 442.595 Q120.112 447.179 120.112 455.928 Q120.112 464.655 117.033 469.262 Q113.978 473.845 108.168 473.845 Q102.358 473.845 99.2789 469.262 Q96.2234 464.655 96.2234 455.928 Q96.2234 447.179 99.2789 442.595 Q102.358 437.989 108.168 437.989 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M132.357 469.239 L148.677 469.239 L148.677 473.174 L126.732 473.174 L126.732 469.239 Q129.394 466.484 133.978 461.854 Q138.584 457.202 139.765 455.859 Q142.01 453.336 142.89 451.6 Q143.793 449.841 143.793 448.151 Q143.793 445.396 141.848 443.66 Q139.927 441.924 136.825 441.924 Q134.626 441.924 132.172 442.688 Q129.742 443.452 126.964 445.003 L126.964 440.28 Q129.788 439.146 132.242 438.567 Q134.695 437.989 136.732 437.989 Q142.103 437.989 145.297 440.674 Q148.492 443.359 148.492 447.85 Q148.492 449.979 147.681 451.901 Q146.894 453.799 144.788 456.391 Q144.209 457.063 141.107 460.28 Q138.006 463.475 132.357 469.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M168.491 441.692 Q164.88 441.692 163.052 445.257 Q161.246 448.799 161.246 455.928 Q161.246 463.035 163.052 466.6 Q164.88 470.141 168.491 470.141 Q172.126 470.141 173.931 466.6 Q175.76 463.035 175.76 455.928 Q175.76 448.799 173.931 445.257 Q172.126 441.692 168.491 441.692 M168.491 437.989 Q174.302 437.989 177.357 442.595 Q180.436 447.179 180.436 455.928 Q180.436 464.655 177.357 469.262 Q174.302 473.845 168.491 473.845 Q162.681 473.845 159.603 469.262 Q156.547 464.655 156.547 455.928 Q156.547 447.179 159.603 442.595 Q162.681 437.989 168.491 437.989 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M63.9319 97.8587 Q60.3208 97.8587 58.4921 101.424 Q56.6865 104.965 56.6865 112.095 Q56.6865 119.201 58.4921 122.766 Q60.3208 126.308 63.9319 126.308 Q67.5661 126.308 69.3717 122.766 Q71.2004 119.201 71.2004 112.095 Q71.2004 104.965 69.3717 101.424 Q67.5661 97.8587 63.9319 97.8587 M63.9319 94.1551 Q69.742 94.1551 72.7976 98.7615 Q75.8763 103.345 75.8763 112.095 Q75.8763 120.822 72.7976 125.428 Q69.742 130.011 63.9319 130.011 Q58.1217 130.011 55.043 125.428 Q51.9875 120.822 51.9875 112.095 Q51.9875 103.345 55.043 98.7615 Q58.1217 94.1551 63.9319 94.1551 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M84.0938 123.46 L88.978 123.46 L88.978 129.34 L84.0938 129.34 L84.0938 123.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M109.163 97.8587 Q105.552 97.8587 103.723 101.424 Q101.918 104.965 101.918 112.095 Q101.918 119.201 103.723 122.766 Q105.552 126.308 109.163 126.308 Q112.797 126.308 114.603 122.766 Q116.432 119.201 116.432 112.095 Q116.432 104.965 114.603 101.424 Q112.797 97.8587 109.163 97.8587 M109.163 94.1551 Q114.973 94.1551 118.029 98.7615 Q121.107 103.345 121.107 112.095 Q121.107 120.822 118.029 125.428 Q114.973 130.011 109.163 130.011 Q103.353 130.011 100.274 125.428 Q97.2187 120.822 97.2187 112.095 Q97.2187 103.345 100.274 98.7615 Q103.353 94.1551 109.163 94.1551 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M133.353 125.405 L149.672 125.405 L149.672 129.34 L127.728 129.34 L127.728 125.405 Q130.39 122.65 134.973 118.021 Q139.58 113.368 140.76 112.025 Q143.006 109.502 143.885 107.766 Q144.788 106.007 144.788 104.317 Q144.788 101.562 142.843 99.8263 Q140.922 98.0902 137.82 98.0902 Q135.621 98.0902 133.168 98.8541 Q130.737 99.618 127.959 101.169 L127.959 96.4467 Q130.783 95.3125 133.237 94.7338 Q135.691 94.1551 137.728 94.1551 Q143.098 94.1551 146.293 96.8402 Q149.487 99.5254 149.487 104.016 Q149.487 106.146 148.677 108.067 Q147.89 109.965 145.783 112.558 Q145.205 113.229 142.103 116.447 Q139.001 119.641 133.353 125.405 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip590)\" d=\"M159.533 94.7801 L177.89 94.7801 L177.89 98.7152 L163.816 98.7152 L163.816 107.187 Q164.834 106.84 165.853 106.678 Q166.871 106.493 167.89 106.493 Q173.677 106.493 177.056 109.664 Q180.436 112.836 180.436 118.252 Q180.436 123.831 176.964 126.933 Q173.491 130.011 167.172 130.011 Q164.996 130.011 162.728 129.641 Q160.482 129.271 158.075 128.53 L158.075 123.831 Q160.158 124.965 162.38 125.521 Q164.603 126.076 167.079 126.076 Q171.084 126.076 173.422 123.97 Q175.76 121.863 175.76 118.252 Q175.76 114.641 173.422 112.535 Q171.084 110.428 167.079 110.428 Q165.204 110.428 163.329 110.845 Q161.478 111.261 159.533 112.141 L159.533 94.7801 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip592)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  276.898,87.9763 382.971,540.292 489.045,144.751 595.118,410.664 701.192,885.885 807.265,988.63 913.339,302.009 1019.41,848.029 1125.49,901.11 1231.56,350.053 \n  1337.63,761.586 1443.71,265.432 1549.78,1222.1 1655.85,724.84 1761.93,1310.76 1868,1445.72 1974.07,671.223 2080.15,1361.27 2186.22,1208.98 2292.29,1244.28 \n  \n  \"/>\n</svg>\n"
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "accuracy(model(x_train), y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9971666666666666"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "accuracy(model(x_valid), y_valid)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9854"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "file_extension": ".jl",
   "name": "julia",
   "mimetype": "application/julia",
   "version": "1.6.2"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.2",
   "language": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}